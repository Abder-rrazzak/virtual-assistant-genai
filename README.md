

# ğŸ§  Generative AI Virtual Assistant

A modular, containerized **virtual assistant** built with **FastAPI**, **LangChain**, and **Docker**.  
This project demonstrates how to integrate **Large Language Models (LLMs)** into a production-ready backend service.

---

## ğŸ“Œ Features

- ğŸš€ **FastAPI backend** â€“ high-performance async API for inference and orchestration  
- ğŸ”— **LangChain integration** â€“ modular chains, tools, and memory support  
- ğŸ³ **Dockerized deployment** â€“ ready to containerize and scale with `docker-compose`  
- ğŸ—„ï¸ **Config-driven architecture** â€“ easily swap models, embeddings, or vector DBs  
- ğŸ§© **Extensible design** â€“ add tools (retrieval, APIs, databases) seamlessly  

---

## ğŸ“‚ Repository Structure

```

generative-ai-assistant/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ main.py               # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ **init**.py
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ **init**.py
â”‚   â”‚       â””â”€â”€ routes.py     # API endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ **init**.py
â”‚   â”‚   â”œâ”€â”€ config.py         # Environment configs
â”‚   â”‚   â””â”€â”€ logging.py        # Logging setup
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ **init**.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py    # LLM wrapper (LangChain/OpenAI)
â”‚   â”‚   â””â”€â”€ assistant_service.py # Business logic / orchestrator
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ **init**.py
â”‚   â”‚   â””â”€â”€ helpers.py        # Reusable utilities
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ **init**.py
â”‚       â””â”€â”€ test_routes.py    # Unit tests
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env.example              # Example env variables

````

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/generative-ai-assistant.git
cd generative-ai-assistant
````

### 2ï¸âƒ£ Create `.env` file

Copy `.env.example` into `.env` and set your environment variables:

```env
OPENAI_API_KEY=your_api_key_here
MODEL_NAME=gpt-4
```

### 3ï¸âƒ£ Install dependencies (local dev)

```bash
python -m venv venv
source venv/bin/activate  # (Linux/macOS)
venv\Scripts\activate     # (Windows)

pip install --upgrade pip
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run locally

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

API will be available at:
ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ³ Run with Docker

### Build & start services

```bash
docker-compose up --build
```

### Stop services

```bash
docker-compose down
```

---

## ğŸ“¡ API Usage

### Example Request

```bash
curl -X POST "http://localhost:8000/api/v1/ask" \
-H "Content-Type: application/json" \
-d '{"query": "Tell me a fun fact about space"}'
```

### Example Response

```json
{
  "response": "Did you know that a day on Venus is longer than a year on Venus?"
}
```

---

## ğŸ§ª Testing

Run tests with `pytest`:

```bash
pytest app/tests/ -v
```

---

## ğŸš€ Roadmap

* [ ] Add memory support (conversation history)
* [ ] Integrate retrieval-augmented generation (RAG)
* [ ] Multi-model support (Anthropic, LLaMA, Mistral)
* [ ] Add monitoring & observability (Prometheus, Grafana)

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a new branch (`feature/my-feature`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to branch (`git push origin feature/my-feature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€“ see [LICENSE](LICENSE) for details.

---
